import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import pickle
import time
from collections import Counter

# Import utils
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
try:
    from model_utils import SentimentLSTM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, clean_text
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import TensorDataset, DataLoader
    HAS_DEPS = True
except ImportError:
    HAS_DEPS = False

def process_dataframe(df, text_col, label_col):
    df = df.dropna(subset=[text_col, label_col])
    
    # 1. X·ª≠ l√Ω Label (QUAN TR·ªåNG: Quy ƒë·ªãnh r√µ r√†ng)
    y_data = []
    # N·∫øu label l√† chu·ªói (Negative/Positive)
    if df[label_col].dtype == object:
        y_data = [1 if str(x).lower() in ['pos', 'positive', 't·ªët', '1'] else 0 for x in df[label_col]]
    # N·∫øu label l√† s·ªë (1-5 sao ho·∫∑c 0-1)
    else:
        # Gi·∫£ s·ª≠ thang 5 sao: >=4 l√† T·ªët (1), <=3 l√† X·∫•u (0)
        # Gi·∫£ s·ª≠ thang 0-1: >0.5 l√† T·ªët
        y_data = [1 if float(x) >= 4 or (float(x) == 1 and df[label_col].max() == 1) else 0 for x in df[label_col]]

    # 2. X·ª≠ l√Ω Text d√πng h√†m chung
    reviews_cleaned = [clean_text(str(r)) for r in df[text_col]]
    
    # 3. T·∫°o b·ªô t·ª´ ƒëi·ªÉn (Vocab)
    all_words = [w for sublist in reviews_cleaned for w in sublist]
    count_words = Counter(all_words)
    # Ch·ªâ l·∫•y t·ª´ xu·∫•t hi·ªán > 1 l·∫ßn ƒë·ªÉ gi·∫£m nhi·ªÖu
    sorted_words = [w for w, c in count_words.most_common() if c > 1]
    vocab = {w: i+1 for i, w in enumerate(sorted_words)}
    
    # 4. Map sang s·ªë
    reviews_int = []
    for words in reviews_cleaned:
        reviews_int.append([vocab.get(w, 0) for w in words])
        
    # 5. Padding
    seq_len = 50
    features = np.zeros((len(reviews_int), seq_len), dtype=int)
    for i, row in enumerate(reviews_int):
        features[i, -min(len(row), seq_len):] = np.array(row)[:seq_len]
        
    X = torch.from_numpy(features)
    y = torch.from_numpy(np.array(y_data)).float()
    
    return X, y, vocab, None

def show():
    st.title("üî• Hu·∫•n luy·ªán Model (Label Fix)")
    
    if not HAS_DEPS: st.error("Thi·∫øu th∆∞ vi·ªán."); return

    # Ch·ªçn file
    data_dir = "data"
    files = [f for f in os.listdir(data_dir) if f.endswith(('.csv', '.xlsx'))] if os.path.exists(data_dir) else []
    
    if not files: st.warning("Kh√¥ng c√≥ file trong data/."); return
    
    col1, col2 = st.columns(2)
    with col1:
        sel_file = st.selectbox("Ch·ªçn file:", files)
        path = os.path.join(data_dir, sel_file)
        df = pd.read_csv(path) if sel_file.endswith('.csv') else pd.read_excel(path)
        st.write(f"ƒê√£ t·∫£i: {len(df)} d√≤ng.")
        
    with col2:
        cols = df.columns.tolist()
        text_col = st.selectbox("C·ªôt n·ªôi dung:", cols)
        label_col = st.selectbox("C·ªôt nh√£n:", cols)
        
    epochs = st.number_input("S·ªë Epochs:", 1, 50, 10)
    
    if st.button("üöÄ Train L·∫°i T·ª´ ƒê·∫ßu"):
        X, y, vocab, err = process_dataframe(df, text_col, label_col)
        
        # Train Loop (R√∫t g·ªçn)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=32, shuffle=True)
        
        vocab_size = len(vocab) + 1
        model = SentimentLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, 1, N_LAYERS).to(device)
        criterion = nn.BCELoss(); optimizer = optim.Adam(model.parameters(), lr=0.005)
        
        bar = st.progress(0)
        model.train()
        
        for e in range(epochs):
            h = model.init_hidden(32, device)
            for inp, lbl in loader:
                if inp.size(0) != 32: continue
                h = tuple([each.data for each in h])
                model.zero_grad()
                out, h = model(inp.to(device), h)
                loss = criterion(out, lbl.to(device))
                loss.backward()
                optimizer.step()
            bar.progress((e+1)/epochs)
            
        # L∆∞u
        if not os.path.exists("models"): os.makedirs("models")
        torch.save(model.state_dict(), "models/sentiment_model.pth")
        with open("models/vocab.pkl", "wb") as f: pickle.dump(vocab, f)
        
        st.success("‚úÖ Train xong! H√£y qua trang Analysis ki·ªÉm tra.")

if __name__ == "__main__": show()
