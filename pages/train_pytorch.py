import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
import time
from collections import Counter

# --- C·∫•u h√¨nh trang (Ch·ªâ ch·∫°y n·∫øu file ch·∫°y ƒë·ªôc l·∫≠p) ---
try:
    if __name__ == "__main__":
        st.set_page_config(page_title="Train PyTorch", layout="wide")
except:
    pass

# --- Import PyTorch ---
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import TensorDataset, DataLoader
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

# ==========================================
# 1. ƒê·ªäNH NGHƒ®A MODEL (SentimentLSTM)
# ==========================================
if HAS_TORCH:
    class SentimentLSTM(nn.Module):
        def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):
            super(SentimentLSTM, self).__init__()
            self.output_dim = output_dim
            self.n_layers = n_layers
            self.hidden_dim = hidden_dim
            
            self.embedding = nn.Embedding(vocab_size, embed_dim)
            self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)
            self.dropout = nn.Dropout(drop_prob)
            self.fc = nn.Linear(hidden_dim, output_dim)
            self.sigmoid = nn.Sigmoid()

        def forward(self, x, hidden):
            batch_size = x.size(0)
            embeds = self.embedding(x)
            lstm_out, hidden = self.lstm(embeds, hidden)
            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)
            
            out = self.dropout(lstm_out)
            out = self.fc(out)
            out = self.sigmoid(out)
            
            out = out.view(batch_size, -1)
            out = out[:, -1]
            return out, hidden

        def init_hidden(self, batch_size, device):
            weight = next(self.parameters()).data
            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),
                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))
            return hidden

# ==========================================
# 2. H√ÄM T·∫†O D·ªÆ LI·ªÜU M·∫™U (FIX L·ªñI THI·∫æU FILE)
# ==========================================
def create_sample_data():
    """T·∫°o file d·ªØ li·ªáu m·∫´u n·∫øu ch∆∞a c√≥"""
    # D·ªØ li·ªáu T√≠ch c·ª±c m·∫´u
    pos_data = """s·∫£n ph·∫©m d√πng r·∫•t t·ªët
ch·∫•t l∆∞·ª£ng tuy·ªát v·ªùi giao h√†ng nhanh
t√¥i r·∫•t th√≠ch s·∫£n ph·∫©m n√†y
ƒë√≥ng g√≥i c·∫©n th·∫≠n ƒë·∫πp m·∫Øt
d√πng r·∫•t b·ªÅn ƒë√°ng ƒë·ªìng ti·ªÅn
nh√¢n vi√™n t∆∞ v·∫•n nhi·ªát t√¨nh
m·ªçi ng∆∞·ªùi n√™n mua nh√©
h√†ng ch√≠nh h√£ng ch·∫•t l∆∞·ª£ng cao
s·ª≠ d·ª•ng m∆∞·ª£t m√† kh√¥ng l·ªói l·∫ßm
ƒë√°nh gi√° 5 sao cho shop
"""
    # D·ªØ li·ªáu Ti√™u c·ª±c m·∫´u
    neg_data = """s·∫£n ph·∫©m qu√° t·ªá
d√πng ƒë∆∞·ª£c v√†i h√¥m ƒë√£ h·ªèng
giao h√†ng ch·∫≠m ch·∫°p th√°i ƒë·ªô l·ªìi l√µm
h√†ng gi·∫£ kh√¥ng gi·ªëng h√¨nh
ƒë·ª´ng mua ph√≠ ti·ªÅn
ch·∫•t l∆∞·ª£ng k√©m qu√° th·∫•t v·ªçng
g·ªçi h·ªó tr·ª£ kh√¥ng ai nghe m√°y
ƒë√≥ng g√≥i s∆° s√†i b·ªã v·ª°
qu·∫£ng c√°o sai s·ª± th·∫≠t
tr·∫£i nghi·ªám t·ªìi t·ªá
"""
    
    with open("train_positive_tokenized.txt", "w", encoding="utf-8") as f:
        f.write(pos_data)
        
    with open("train_negative_tokenized.txt", "w", encoding="utf-8") as f:
        f.write(neg_data)

# ==========================================
# 3. H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU
# ==========================================
def read_txt(file_path):
    reviews = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    reviews.append(line)
    return reviews

def preprocess_data():
    # ƒê·ªçc d·ªØ li·ªáu
    pos_reviews = read_txt("train_positive_tokenized.txt")
    neg_reviews = read_txt("train_negative_tokenized.txt")
    
    if not pos_reviews or not neg_reviews:
        return None, None, None, "D·ªØ li·ªáu r·ªóng."

    reviews = pos_reviews + neg_reviews
    labels = [1]*len(pos_reviews) + [0]*len(neg_reviews)

    # T·∫°o Vocab
    words = []
    for r in reviews:
        words.extend(r.split())
    
    count_words = Counter(words)
    sorted_words = count_words.most_common(len(count_words))
    vocab_to_int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}
    
    # M√£ h√≥a reviews
    reviews_int = []
    for r in reviews:
        r_int = [vocab_to_int.get(w, 0) for w in r.split()] # D√πng .get ƒë·ªÉ tr√°nh l·ªói key
        reviews_int.append(r_int)
        
    # Padding
    seq_len = 50
    features = np.zeros((len(reviews_int), seq_len), dtype=int)
    for i, row in enumerate(reviews_int):
        if len(row) > 0:
            features[i, -min(len(row), seq_len):] = np.array(row)[:seq_len]

    # Tensor
    X = torch.from_numpy(features)
    y = torch.from_numpy(np.array(labels)).float()

    return X, y, vocab_to_int, None

# ==========================================
# 4. GIAO DI·ªÜN CH√çNH (H√†m show)
# ==========================================
def show():
    st.markdown("""
    <style>
    div.stButton > button {background-color: #ff4b4b; color: white; width: 100%; font-weight: bold;}
    </style>
    """, unsafe_allow_html=True)

    st.markdown('<div style="background-color:rgba(255,255,255,0.9); padding:20px; border-radius:15px;">', unsafe_allow_html=True)
    st.title("üî• Hu·∫•n luy·ªán Model LSTM (PyTorch)")

    if not HAS_TORCH:
        st.error("‚ö†Ô∏è Ch∆∞a c√†i ƒë·∫∑t th∆∞ vi·ªán `torch`. Vui l√≤ng ch·∫°y `pip install torch`.")
        st.markdown('</div>', unsafe_allow_html=True)
        return

    # --- KI·ªÇM TRA & T·∫†O DATA ---
    file_exists = os.path.exists("train_positive_tokenized.txt") and os.path.exists("train_negative_tokenized.txt")
    
    if not file_exists:
        st.warning("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu hu·∫•n luy·ªán.")
        st.info("B·∫°n c√≥ mu·ªën t·∫°o d·ªØ li·ªáu m·∫´u (Sample Data) ƒë·ªÉ ch·∫°y th·ª≠ kh√¥ng?")
        
        if st.button("üõ†Ô∏è T·∫°o D·ªØ Li·ªáu M·∫´u & Ti·∫øp T·ª•c"):
            create_sample_data()
            st.success("‚úÖ ƒê√£ t·∫°o file th√†nh c√¥ng! Vui l√≤ng ƒë·ª£i trang t·∫£i l·∫°i...")
            time.sleep(1)
            st.rerun() # T·ª± ƒë·ªông load l·∫°i trang
        
        st.markdown('</div>', unsafe_allow_html=True)
        return # D·ª´ng h√†m t·∫°i ƒë√¢y n·∫øu ch∆∞a c√≥ file

    # --- N·∫æU ƒê√É C√ì DATA TH√å HI·ªÜN GIAO DI·ªÜN TRAIN ---
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("‚öôÔ∏è Tham s·ªë")
        epochs = st.number_input("Epochs", 1, 100, 10) # TƒÉng default epoch l√™n 10 v√¨ data √≠t
        batch_size = st.selectbox("Batch Size", [2, 4, 16, 32], index=1) # Gi·∫£m batch size v√¨ data m·∫´u √≠t
        lr = st.select_slider("Learning Rate", options=[0.01, 0.005, 0.001], value=0.005)
        
        btn_train = st.button("üöÄ B·∫Øt ƒë·∫ßu Train")

    with col2:
        st.subheader("üìà Ti·∫øn tr√¨nh")
        log_area = st.empty()
        chart_loss = st.empty()
        status_text = st.empty()
        
        if btn_train:
            status_text.info("üîÑ ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")
            X, y, vocab, err = preprocess_data()
            
            if err:
                st.error(err)
            else:
                # Setup Training
                # Data m·∫´u √≠t n√™n batch_size ph·∫£i nh·ªè h∆°n len(data)
                curr_batch = min(batch_size, len(X))
                
                train_data = TensorDataset(X, y)
                train_loader = DataLoader(train_data, shuffle=True, batch_size=curr_batch, drop_last=False)
                
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                status_text.info(f"üíª Device: **{device}** | Vocab: {len(vocab)} t·ª´ | Samples: {len(X)}")
                
                # Model Init
                vocab_size = len(vocab) + 1
                embedding_dim = 400
                hidden_dim = 256
                n_layers = 2
                
                model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, 1, n_layers)
                model.to(device)
                
                criterion = nn.BCELoss()
                optimizer = optim.Adam(model.parameters(), lr=lr)
                
                # Loop
                model.train()
                loss_history = []
                progress_bar = st.progress(0)
                
                start_time = time.time()
                
                for e in range(epochs):
                    h = model.init_hidden(curr_batch, device)
                    avg_loss = []
                    
                    for inputs, labels in train_loader:
                        # Handle batch size dynamic (n·∫øu batch cu·ªëi l·∫ª)
                        current_batch_size = inputs.size(0)
                        h = model.init_hidden(current_batch_size, device) # Re-init hidden v·ªõi ƒë√∫ng k√≠ch th∆∞·ªõc batch
                        
                        inputs, labels = inputs.to(device), labels.to(device)
                        
                        model.zero_grad()
                        output, h = model(inputs, h)
                        
                        loss = criterion(output, labels)
                        loss.backward()
                        nn.utils.clip_grad_norm_(model.parameters(), 5)
                        optimizer.step()
                        
                        avg_loss.append(loss.item())
                    
                    epoch_loss = np.mean(avg_loss) if avg_loss else 0
                    loss_history.append(epoch_loss)
                    
                    chart_loss.line_chart(loss_history)
                    log_area.text(f"Epoch {e+1}/{epochs} | Loss: {epoch_loss:.5f}")
                    progress_bar.progress((e + 1) / epochs)
                
                # Save
                if not os.path.exists("models"):
                    os.makedirs("models")
                
                torch.save(model.state_dict(), "models/sentiment_model.pth")
                with open("models/vocab.pkl", "wb") as f:
                    pickle.dump(vocab, f)
                
                st.balloons()
                status_text.success(f"‚úÖ Xong! Model ƒë√£ l∆∞u v√†o `models/`.")
                st.info("üëâ H√£y qua trang **Analysis** ƒë·ªÉ test th·ª≠.")

    st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    show()
