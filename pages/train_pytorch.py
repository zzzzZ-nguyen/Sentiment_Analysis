import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import pickle
from collections import Counter

# ==========================================
# 1. C·∫§U H√åNH (CONFIG)
# ==========================================
DATA_DIR = "data"  # T√™n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu

# ƒê∆∞·ªùng d·∫´n c√°c file Train
TRAIN_FILES = {
    0: os.path.join(DATA_DIR, "train_negative_tokenized.txt"),
    1: os.path.join(DATA_DIR, "train_neutral_tokenized.txt"),
    2: os.path.join(DATA_DIR, "train_positive_tokenized.txt")
}

# ƒê∆∞·ªùng d·∫´n file Test (ƒê·ªÉ ki·ªÉm tra ƒë·ªô ch√≠nh x√°c)
TEST_FILE = os.path.join(DATA_DIR, "test_tokenized_ANS.txt")

# N∆°i l∆∞u model
MODEL_SAVE_PATH = "models/sentiment_model.pth"
VOCAB_SAVE_PATH = "models/vocab.pkl"

# Hyperparameters
EMBED_DIM = 100
HIDDEN_DIM = 128
OUTPUT_DIM = 3
LEARNING_RATE = 0.001
EPOCHS = 15
BATCH_SIZE = 32

# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
if not os.path.exists("models"):
    os.makedirs("models")

# ==========================================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU
# ==========================================
def read_train_data():
    """ƒê·ªçc 3 file train ri√™ng bi·ªát"""
    texts = []
    labels = []
    print("\n--- 1. ƒêang ƒë·ªçc d·ªØ li·ªáu hu·∫•n luy·ªán (Train) ---")
    for label, filepath in TRAIN_FILES.items():
        if os.path.exists(filepath):
            count = 0
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        texts.append(line)
                        labels.append(label)
                        count += 1
            print(f"   - ƒê√£ ƒë·ªçc {os.path.basename(filepath)}: {count} d√≤ng.")
        else:
            print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file {filepath}")
    return texts, labels

def read_test_data():
    """ƒê·ªçc file test ƒë·∫∑c bi·ªát (D√≤ng 1: Text, D√≤ng 2: Label)"""
    texts = []
    labels = []
    print("\n--- 2. ƒêang ƒë·ªçc d·ªØ li·ªáu ki·ªÉm th·ª≠ (Test) ---")
    
    if os.path.exists(TEST_FILE):
        with open(TEST_FILE, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
            
        # File test c√≥ d·∫°ng: D√≤ng ch·∫µn l√† Text, D√≤ng l·∫ª l√† Label (NEG, POS, NEU)
        for i in range(0, len(lines) - 1, 2):
            text = lines[i].strip()
            label_str = lines[i+1].strip()
            
            # Chuy·ªÉn label ch·ªØ sang s·ªë
            if label_str == 'NEG': label = 0
            elif label_str == 'NEU': label = 1
            elif label_str == 'POS': label = 2
            else: continue # B·ªè qua n·∫øu l·ªói
            
            if text:
                texts.append(text)
                labels.append(label)
        print(f"   - ƒê√£ ƒë·ªçc file Test: {len(texts)} d√≤ng.")
    else:
        print(f"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file Test t·∫°i {TEST_FILE}")
        
    return texts, labels

def build_vocab(texts):
    print("\n--- 3. ƒêang x√¢y d·ª±ng b·ªô t·ª´ ƒëi·ªÉn ---")
    words = []
    for text in texts:
        words.extend(text.lower().split())
    
    count = Counter(words)
    vocab = {'<PAD>': 0, '<UNK>': 1}
    idx = 2
    for word, c in count.most_common():
        if c > 1: # Ch·ªâ l·∫•y t·ª´ xu·∫•t hi·ªán > 1 l·∫ßn
            vocab[word] = idx
            idx += 1
    return vocab

def text_to_indices(text, vocab, max_len=50):
    words = text.lower().split()
    indices = [vocab.get(w, vocab['<UNK>']) for w in words]
    if len(indices) < max_len:
        indices += [vocab['<PAD>']] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    return indices

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, vocab):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        vec = text_to_indices(self.texts[idx], self.vocab)
        return torch.tensor(vec, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)

# ==========================================
# 3. M√î H√åNH LSTM
# ==========================================
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim) 
        self.dropout = nn.Dropout(0.4)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # Gh√©p hidden state c·ªßa 2 chi·ªÅu
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        out = self.fc(self.dropout(hidden))
        return out

# ==========================================
# 4. CH·∫†Y HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å
# ==========================================
def train():
    # Load Data
    train_texts, train_labels = read_train_data()
    test_texts, test_labels = read_test_data()
    
    if not train_texts:
        print("‚ùå L·ªói: Kh√¥ng c√≥ d·ªØ li·ªáu Train.")
        return

    # Build Vocab (D·ª±a tr√™n c·∫£ t·∫≠p Train v√† Test ƒë·ªÉ kh√¥ng b·ªã s√≥t t·ª´)
    vocab = build_vocab(train_texts + test_texts)
    print(f"   - K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn: {len(vocab)} t·ª´")

    # Prepare Datasets
    train_dataset = SentimentDataset(train_texts, train_labels, vocab)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # Setup Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu Train tr√™n thi·∫øt b·ªã: {device}")
    
    model = LSTMClassifier(len(vocab), EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # --- TRAINING LOOP ---
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        correct = 0
        total = 0
        
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
            
        acc = 100 * correct / total
        print(f"Epoch [{epoch+1}/{EPOCHS}] | Loss: {total_loss/len(train_loader):.4f} | Train Acc: {acc:.2f}%")

    # --- EVALUATION ON TEST SET (Quan tr·ªçng) ---
    print("\n--- üìä ƒê√°nh gi√° tr√™n t·∫≠p Test (D·ªØ li·ªáu ch∆∞a t·ª´ng h·ªçc) ---")
    if test_texts:
        test_dataset = SentimentDataset(test_texts, test_labels, vocab)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ch·∫•m ƒëi·ªÉm
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                test_total += targets.size(0)
                test_correct += (predicted == targets).sum().item()
        
        print(f"üéØ ƒê·ªò CH√çNH X√ÅC TH·ª∞C T·∫æ (TEST ACCURACY): {100 * test_correct / test_total:.2f}%")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ file test ƒë·ªÉ ƒë√°nh gi√°.")

    # Save Model
    print("\n--- üíæ ƒêang l∆∞u model... ---")
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    with open(VOCAB_SAVE_PATH, 'wb') as f:
        pickle.dump(vocab, f)
    print(f"‚úÖ ƒê√£ xong! File model t·∫°i: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    train()
