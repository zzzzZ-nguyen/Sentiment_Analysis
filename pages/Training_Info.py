import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ==================================================
# üì¶ LOAD MODEL OBJECTS
# ==================================================
@st.cache_resource
def load_model_objects():
    # ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi cho ph√π h·ª£p v·ªõi c·∫•u tr√∫c th∆∞ m·ª•c c·ªßa b·∫°n
    model_path = os.path.join("models", "model_en.pkl")
    vectorizer_path = os.path.join("models", "vectorizer_en.pkl")

    try:
        model = joblib.load(model_path)
        vectorizer = joblib.load(vectorizer_path)
        return model, vectorizer
    except:
        return None, None

# ==================================================
# üìä TRAINING INFO ‚Äì SENTIMENT ANALYSIS
# ==================================================
def show():
    st.markdown(
        "<h3 style='color:#2b6f3e;'>Training Info ‚Äì Sentiment Analysis (Advanced)</h3>",
        unsafe_allow_html=True
    )

    st.write(
        "This section presents the training pipeline, model information, "
        "evaluation results, and comparison of sentiment analysis models."
    )
    st.write("---")

    # Load Model
    model, vectorizer = load_model_objects()

    # ==================================================
    # 1Ô∏è‚É£ RAW DATASET (M·ªü r·ªông d·ªØ li·ªáu m·∫´u ƒë·ªÉ t√≠nh to√°n th·∫≠t)
    # ==================================================
    st.subheader("1Ô∏è‚É£ Raw Dataset")

    # T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p ƒë·ªß l·ªõn ƒë·ªÉ demo t√≠nh to√°n
    raw_data = pd.DataFrame({
        "review": [
            "S·∫£n ph·∫©m r·∫•t t·ªët", "Ch·∫•t l∆∞·ª£ng k√©m, th·∫•t v·ªçng", "This product is amazing", 
            "Bad quality, waste of money", "Average product", "Really loved it",
            "Terrible experience", "Normal quality", "Excellent service", "Don't buy this"
        ],
        "label": [
            "positive", "negative", "positive", 
            "negative", "neutral", "positive",
            "negative", "neutral", "positive", "negative"
        ]
    })

    st.dataframe(raw_data)
    st.caption("‚Ä¢ Dataset m·∫´u ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ demo t√≠nh to√°n c√°c ch·ªâ s·ªë b√™n d∆∞·ªõi.")
    st.write("---")

    # ==================================================
    # 2Ô∏è‚É£ PREPROCESSING
    # ==================================================
    st.subheader("2Ô∏è‚É£ Preprocessed Data")
    processed_data = raw_data.copy()
    processed_data["review_clean"] = processed_data["review"].str.lower()
    st.dataframe(processed_data.head())
    st.caption("Ti·ªÅn x·ª≠ l√Ω: Chuy·ªÉn ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát.")
    st.write("---")

    # ==================================================
    # 3Ô∏è‚É£ MODEL INFORMATION
    # ==================================================
    st.subheader("3Ô∏è‚É£ Model Information")
    
    if model and vectorizer:
        col_info1, col_info2 = st.columns(2)
        with col_info1:
            st.markdown("##### üìå Logistic Regression Config")
            st.table(pd.DataFrame({
                "Property": ["Model Type", "Classes", "Solver"],
                "Value": ["LogisticRegression", str(model.classes_), getattr(model, 'solver', 'lbfgs')]
            }))
        
        with col_info2:
            st.markdown("##### üìå TF-IDF Config")
            st.table(pd.DataFrame({
                "Property": ["Vectorizer", "Vocab Size", "N-gram"],
                "Value": ["TfidfVectorizer", len(vectorizer.vocabulary_), str(vectorizer.ngram_range)]
            }))
    else:
        st.error("Kh√¥ng t√¨m th·∫•y file model trong th∆∞ m·ª•c 'models/'. Vui l√≤ng ki·ªÉm tra l·∫°i.")

    st.write("---")

    # ==================================================
    # 4Ô∏è‚É£ TRAINING RESULTS (N√ÇNG C·∫§P: T√çNH TO√ÅN T·ª∞ ƒê·ªòNG)
    # ==================================================
    st.subheader("4Ô∏è‚É£ Training Results (Real-time Calculation)")

    if model and vectorizer:
        # --- T·ª± ƒë·ªông d·ª± ƒëo√°n v√† t√≠nh ƒëi·ªÉm ---
        X_test = vectorizer.transform(processed_data["review_clean"])
        y_true = processed_data["label"]
        y_pred = model.predict(X_test)

        # T√≠nh ch·ªâ s·ªë th·∫≠t
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)

        # Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£ (ƒê√£ t√≠nh to√°n)
        results = pd.DataFrame({
            "Metric": ["Accuracy", "Precision", "Recall", "F1-score"],
            "Score": [acc, prec, recall_score(y_true, y_pred, average='weighted', zero_division=0), f1]
        })
        st.table(results)

        # --- N√ÇNG C·∫§P: V·∫º BI·ªÇU ƒê·ªí CONFUSION MATRIX ---
        st.markdown("**üìä Visualizations**")
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            st.write("*Confusion Matrix:*")
            fig_cm, ax_cm = plt.subplots(figsize=(4, 3))
            cm = confusion_matrix(y_true, y_pred, labels=model.classes_)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', 
                        xticklabels=model.classes_, yticklabels=model.classes_)
            plt.xlabel('Predicted')
            plt.ylabel('True')
            st.pyplot(fig_cm)

        with col_viz2:
            st.write("*WordCloud (Feature Visualization):*")
            text_wc = " ".join(processed_data["review_clean"])
            wc = WordCloud(width=400, height=300, background_color='white', colormap='tab10').generate(text_wc)
            fig_wc, ax_wc = plt.subplots(figsize=(4, 3))
            ax_wc.imshow(wc, interpolation='bilinear')
            ax_wc.axis("off")
            st.pyplot(fig_wc)

    st.write("---")

    # ==================================================
    # 5Ô∏è‚É£ MODEL CONFIDENCE (N√ÇNG C·∫§P)
    # ==================================================
    st.subheader("5Ô∏è‚É£ Model Confidence Evaluation")

    if model and vectorizer:
        # L·∫•y x√°c su·∫•t d·ª± ƒëo√°n (Confidence score)
        probs = model.predict_proba(X_test)
        max_probs = np.max(probs, axis=1)
        
        confidence_df = pd.DataFrame({
            "Review": processed_data["review"],
            "Predicted": y_pred,
            "Confidence": max_probs
        })
        
        # Format hi·ªÉn th·ªã m√†u cho c·ªôt Confidence
        st.dataframe(confidence_df.style.background_gradient(subset=["Confidence"], cmap="Greens"))

    st.write("---")

    # ==================================================
    # 6Ô∏è‚É£ CONCLUSION (Gi·ªØ nguy√™n)
    # ==================================================
    st.subheader("6Ô∏è‚É£ Conclusion & Future Work")
    st.markdown(
        """
        **Conclusion:**
        - Model ƒë∆∞·ª£c load tr·ª±c ti·∫øp v√† t√≠nh to√°n realtime.
        - H·ªá th·ªëng t√≠ch h·ª£p Visualization (Bi·ªÉu ƒë·ªì) gi√∫p d·ªÖ d√†ng ƒë√°nh gi√°.

        **Future Work:**
        - M·ªü r·ªông dataset.
        - √Åp d·ª•ng Transformer (BERT, PhoBERT).
        """
    )
