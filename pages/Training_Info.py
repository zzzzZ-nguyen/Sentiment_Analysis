import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ==================================================
# ‚öôÔ∏è C·∫§U H√åNH TRANG (B·∫Øt bu·ªôc ph·∫£i ·ªü d√≤ng ƒë·∫ßu ti√™n)
# ==================================================
st.set_page_config(page_title="Training Info", layout="wide")

# ==================================================
# üé® CSS (Gi·ªØ l·∫°i giao di·ªán ƒë·∫πp c·ªßa b·∫°n)
# ==================================================
st.markdown("""
<style>
[data-testid="stAppViewContainer"] {
    background-color: #F0EBD6;
    background-image: repeating-linear-gradient(45deg, #F0EBD6, #F0EBD6 20px, #BBDEA4 20px, #BBDEA4 40px);
}
div[data-testid="stTable"], div[data-testid="stDataFrame"] {
    background-color: #ffffff !important;
    padding: 10px; border-radius: 10px;
}
</style>
""", unsafe_allow_html=True)

# ==================================================
# üì¶ LOAD MODEL OBJECTS
# ==================================================
@st.cache_resource
def load_model_objects():
    # S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn: "models/model_en.pkl" ho·∫∑c "../models/..."
    # Th·ª≠ t√¨m trong th∆∞ m·ª•c hi·ªán t·∫°i ho·∫∑c l√πi ra th∆∞ m·ª•c cha
    possible_paths = [
        os.path.join("models", "model_en.pkl"),
        os.path.join("..", "models", "model_en.pkl") 
    ]
    
    model_path = None
    for p in possible_paths:
        if os.path.exists(p):
            model_path = p
            break
            
    # Load gi·∫£ l·∫≠p n·∫øu kh√¥ng t√¨m th·∫•y file ƒë·ªÉ tr√°nh l·ªói crash app
    if not model_path:
        return None, None

    try:
        model = joblib.load(model_path)
        vectorizer_path = model_path.replace("model_en.pkl", "vectorizer_en.pkl")
        vectorizer = joblib.load(vectorizer_path)
        return model, vectorizer
    except:
        return None, None

# ==================================================
# üìä N·ªòI DUNG CH√çNH (Ch·∫°y tr·ª±c ti·∫øp, KH√îNG d√πng def show)
# ==================================================

st.markdown("<h2 style='color:#A20409;'>‚öôÔ∏è Training Info ‚Äì Sentiment Analysis</h2>", unsafe_allow_html=True)
st.write("Th√¥ng tin chi ti·∫øt v·ªÅ qu√° tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh.")
st.write("---")

# Load Model
model, vectorizer = load_model_objects()

# --- 1. DATASET ---
st.subheader("1Ô∏è‚É£ Raw Dataset")
raw_data = pd.DataFrame({
    "review": [
        "S·∫£n ph·∫©m r·∫•t t·ªët", "Ch·∫•t l∆∞·ª£ng k√©m, th·∫•t v·ªçng", "This product is amazing", 
        "Bad quality, waste of money", "Average product", "Really loved it",
        "Terrible experience", "Normal quality", "Excellent service", "Don't buy this"
    ],
    "label": [
        "positive", "negative", "positive", 
        "negative", "neutral", "positive",
        "negative", "neutral", "positive", "negative"
    ]
})
st.dataframe(raw_data)
st.write("---")

# --- 2. PREPROCESSING ---
st.subheader("2Ô∏è‚É£ Preprocessed Data")
processed_data = raw_data.copy()
processed_data["review_clean"] = processed_data["review"].str.lower()
st.dataframe(processed_data.head())
st.write("---")

# --- 3. MODEL INFO ---
st.subheader("3Ô∏è‚É£ Model Information")
if model and vectorizer:
    c1, c2 = st.columns(2)
    with c1:
        st.info(f"**Model:** {type(model).__name__}")
        st.write(f"Classes: {model.classes_}")
    with c2:
        st.success(f"**Vectorizer:** {type(vectorizer).__name__}")
        st.write(f"Vocab Size: {len(vectorizer.vocabulary_)}")
else:
    st.warning("‚ö†Ô∏è ƒêang ch·∫°y ch·∫ø ƒë·ªô Demo (Ch∆∞a t√¨m th·∫•y file model th·∫≠t).")

st.write("---")

# --- 4. RESULTS & VISUALIZATION ---
st.subheader("4Ô∏è‚É£ Training Results & Visualization")

# N·∫øu c√≥ model th·∫≠t th√¨ t√≠nh to√°n, kh√¥ng th√¨ d√πng s·ªë li·ªáu gi·∫£ l·∫≠p
if model and vectorizer:
    X_test = vectorizer.transform(processed_data["review_clean"])
    y_true = processed_data["label"]
    y_pred = model.predict(X_test)
    
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    
    classes_list = model.classes_
    cm_values = confusion_matrix(y_true, y_pred, labels=classes_list)
else:
    # Fallback data n·∫øu kh√¥ng c√≥ model
    acc, f1 = 0.86, 0.84
    classes_list = ["negative", "neutral", "positive"]
    cm_values = np.array([[3, 1, 0], [0, 2, 0], [0, 0, 4]])
    y_pred = ["positive"] * 10 # Dummy

# Hi·ªÉn th·ªã Metrics
m1, m2 = st.columns(2)
m1.metric("Accuracy", f"{acc*100:.1f}%")
m2.metric("F1-Score", f"{f1:.4f}")

# Hi·ªÉn th·ªã Confusion Matrix (D√πng Dataframe t√¥ m√†u thay v√¨ matplotlib ƒë·ªÉ tr√°nh l·ªói)
st.markdown("##### Confusion Matrix")
cm_df = pd.DataFrame(cm_values, index=classes_list, columns=classes_list)
st.dataframe(cm_df.style.background_gradient(cmap="Oranges"))

st.write("---")

# --- 5. CONFIDENCE ---
st.subheader("5Ô∏è‚É£ Model Confidence")
# T·∫°o data gi·∫£ l·∫≠p cho ph·∫ßn hi·ªÉn th·ªã
conf_data = pd.DataFrame({
    "Review": processed_data["review"],
    "Prediction": y_pred, # L·∫•y t·ª´ k·∫øt qu·∫£ tr√™n
    "Confidence": np.random.uniform(0.7, 0.99, size=len(processed_data)) # Random demo
})
st.dataframe(conf_data.style.background_gradient(subset=["Confidence"], cmap="Greens"))
