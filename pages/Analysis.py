import streamlit as st
import torch
import torch.nn as nn
import pickle
import os
import re

# ƒê·ªãnh nghƒ©a l·∫°i class LSTM ƒë·ªÉ load ƒë∆∞·ª£c model
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim) 
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        out = self.fc(self.dropout(hidden))
        return out

@st.cache_resource
def load_pytorch_model():
    vocab_path = "models/vocab.pkl"
    model_path = "models/sentiment_model.pth"
    
    if not os.path.exists(vocab_path) or not os.path.exists(model_path):
        return None, None
        
    # Load Vocab
    with open(vocab_path, 'rb') as f:
        vocab = pickle.load(f)
        
    # Init Model
    device = torch.device('cpu')
    model = LSTMClassifier(len(vocab), 100, 128, 3)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model, vocab

def text_to_tensor(text, vocab, max_len=50):
    words = text.lower().split()
    indices = [vocab.get(w, vocab.get('<UNK>', 1)) for w in words]
    if len(indices) < max_len:
        indices += [vocab.get('<PAD>', 0)] * (max_len - len(indices))
    else:
        indices = indices[:max_len]
    return torch.tensor([indices], dtype=torch.long)

# ==========================================
# MAIN FUNCTION (ƒê∆∞·ª£c g·ªçi t·ª´ app.py)
# ==========================================
def show():
    st.markdown("<h2 style='color:#2b6f3e;'>üß† Ph√¢n T√≠ch C·∫£m X√∫c (Deep Learning)</h2>", unsafe_allow_html=True)
    st.write("S·ª≠ d·ª•ng m√¥ h√¨nh LSTM ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c (Ti·∫øng Vi·ªát/Anh).")

    model, vocab = load_pytorch_model()
    
    if not model:
        st.warning("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y model. H√£y ch·∫°y `python train_pytorch.py` tr∆∞·ªõc!")
        return

    col1, col2 = st.columns([2, 1])
    
    with col1:
        user_input = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n s·∫£n ph·∫©m:", height=150, placeholder="V√≠ d·ª•: S·∫£n ph·∫©m d√πng r·∫•t t·ªët, giao h√†ng nhanh...")
        
        if st.button("üöÄ Ph√¢n T√≠ch Ngay", type="primary"):
            if user_input.strip():
                # D·ª± ƒëo√°n
                tensor_input = text_to_tensor(user_input, vocab)
                with torch.no_grad():
                    outputs = model(tensor_input)
                    probs = torch.softmax(outputs, dim=1)
                    max_prob, predicted_class = torch.max(probs, 1)
                
                # Mapping k·∫øt qu·∫£ (0: Neg, 1: Neu, 2: Pos - D·ª±a theo code train)
                labels = {0: "Ti√™u c·ª±c (Negative)", 1: "Trung t√≠nh (Neutral)", 2: "T√≠ch c·ª±c (Positive)"}
                colors = {0: "error", 1: "warning", 2: "success"}
                
                pred_label = labels[predicted_class.item()]
                conf = max_prob.item()
                
                # Hi·ªÉn th·ªã
                st.divider()
                msg_func = getattr(st, colors[predicted_class.item()])
                msg_func(f"K·∫øt qu·∫£: **{pred_label}**")
                st.info(f"ƒê·ªô tin c·∫≠y: **{conf:.2%}**")
            else:
                st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung.")

    with col2:
        st.info("‚ÑπÔ∏è **H∆∞·ªõng d·∫´n:**\n\nNh·∫≠p m·ªôt c√¢u b√¨nh lu·∫≠n v·ªÅ s·∫£n ph·∫©m (ƒëi·ªán tho·∫°i, m√°y t√≠nh, v.v.) ƒë·ªÉ xem m√°y t√≠nh ƒë√°nh gi√° c·∫£m x√∫c nh∆∞ th·∫ø n√†o.")
